{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dtkachenko/.pyenv/versions/3.6.3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Input, Dense, Flatten, Lambda, merge\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-02-11 01:58:09,386] Making new env: BreakoutDeterministic-v4\n"
     ]
    }
   ],
   "source": [
    "# The Deterministic-v4 version of the Atari environments is exactly equivalent to what DeepMind used in their paper.\n",
    "# Specifically, \"-v4\" essentially implements frame skipping so that the agent acts on every 4th frame. \n",
    "# This reduces the number of frames to process and thus reduces training time \n",
    "# without sacrificing agent's game performance.\n",
    "env = gym.make('BreakoutDeterministic-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 50000\n",
    "MODEL_PATH = \"model/breakout_dueling_ddqn.h5\"\n",
    "TENSORBOARD_LOG_PATH = 'summary/breakout_dueling_ddqn'\n",
    "MODEL_CHECKPOINT_FREQUENCY = 100\n",
    "\n",
    "# actions:\n",
    "NO_ACTION_NO_BALL = 0\n",
    "NO_ACTION = 1\n",
    "MOVE_RIGHT = 2\n",
    "MOVE_LEFT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 210x160x3 (RGB color) -> 84x84 (grayscale)\n",
    "# also transforms floats to 8-bit ints to conserve memory\n",
    "def preprocess(observed_state):\n",
    "    return np.uint8(resize(rgb2gray(observed_state), (84, 84), mode='constant') * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_state_history(observed_state):\n",
    "    # No preceding frames at the start of an episode, so just copy 4 times\n",
    "    state = preprocess(observed_state)\n",
    "    history = np.stack((state, state, state, state), axis=2)\n",
    "    history = np.reshape([history], (1, 84, 84, 4))\n",
    "    return state, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDDQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "\n",
    "        # environment settings\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # epsilon-greedy policy parameters\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.1\n",
    "        self.exploration_steps = 1000000.\n",
    "        self.epsilon_decay_step = (self.epsilon_start - self.epsilon_end) / self.exploration_steps\n",
    "\n",
    "        # training parameters\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 50000\n",
    "        self.update_target_rate = 10000\n",
    "        self.discount_factor = 0.99\n",
    "        self.memory = deque(maxlen=400000)\n",
    "        self.no_op_steps = 30\n",
    "\n",
    "        # build\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "        self.optimizer = self.optimizer()\n",
    "\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        K.set_session(self.sess)\n",
    "\n",
    "        self.avg_q_max, self.avg_loss = 0, 0\n",
    "        self.summary_placeholders, self.update_ops, self.summary_op = self.setup_summary()\n",
    "        self.summary_writer = tf.summary.FileWriter(TENSORBOARD_LOG_PATH, self.sess.graph)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(MODEL_PATH)\n",
    "\n",
    "    # if cost is in [-1; 1] cost is quadratic to error, otherwise â€“ linear\n",
    "    def optimizer(self):\n",
    "        a = K.placeholder(shape=(None,), dtype='int32')\n",
    "        y = K.placeholder(shape=(None,), dtype='float32')\n",
    "\n",
    "        py_x = self.model.output\n",
    "\n",
    "        a_one_hot = K.one_hot(a, self.action_size)\n",
    "        q_value = K.sum(py_x * a_one_hot, axis=1)\n",
    "        error = K.abs(y - q_value)\n",
    "\n",
    "        quadratic_part = K.clip(error, 0.0, 1.0)\n",
    "        linear_part = error - quadratic_part\n",
    "        loss = K.mean(0.5 * K.square(quadratic_part) + linear_part)\n",
    "\n",
    "        optimizer = RMSprop(lr=0.00025, epsilon=0.01)\n",
    "        updates = optimizer.get_updates(self.model.trainable_weights, [], loss)\n",
    "        train = K.function([self.model.input, a, y], [loss], updates=updates)\n",
    "\n",
    "        return train\n",
    "\n",
    "    # frame representation -> CNN -> advantage/value streams -> merge two streams -> Q-value of each action\n",
    "    def build_model(self):\n",
    "        input = Input(shape=self.state_size)\n",
    "        shared = Conv2D(32, (8, 8), strides=(4, 4), activation='relu')(input)\n",
    "        shared = Conv2D(64, (4, 4), strides=(2, 2), activation='relu')(shared)\n",
    "        shared = Conv2D(64, (3, 3), strides=(1, 1), activation='relu')(shared)\n",
    "        flatten = Flatten()(shared)\n",
    "\n",
    "        # stream that estimates state-dependent action advantages\n",
    "        advantage_fc = Dense(512, activation='relu')(flatten)\n",
    "        advantage = Dense(self.action_size)(advantage_fc)\n",
    "        advantage = Lambda(lambda a: a[:, :] - K.mean(a[:, :], keepdims=True),\n",
    "                           output_shape=(self.action_size,))(advantage)\n",
    "\n",
    "        # stream which estimates state values\n",
    "        value_fc = Dense(512, activation='relu')(flatten)\n",
    "        value = Dense(1)(value_fc)\n",
    "        value = Lambda(lambda s: K.expand_dims(s[:, 0], -1),\n",
    "                       output_shape=(self.action_size,))(value)\n",
    "\n",
    "        # merge two streams to produce Q-value\n",
    "        q_value = merge([value, advantage], mode='sum')\n",
    "        model = Model(inputs=input, outputs=q_value)\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    # epsilon-greedy policy\n",
    "    def get_action(self, history):\n",
    "        history = np.float32(history / 255.0)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(history)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    # save sample <s, a, r, s'> to the replay memory\n",
    "    def replay_memory(self, history, action, reward, next_history, dead):\n",
    "        self.memory.append((history, action, reward, next_history, dead))\n",
    "\n",
    "    # sample from replay memory (batches of specified size)\n",
    "    def train_replay(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon -= self.epsilon_decay_step\n",
    "\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        history = np.zeros((self.batch_size, self.state_size[0],\n",
    "                            self.state_size[1], self.state_size[2]))\n",
    "        next_history = np.zeros((self.batch_size, self.state_size[0],\n",
    "                                 self.state_size[1], self.state_size[2]))\n",
    "        target = np.zeros((self.batch_size,))\n",
    "        action, reward, dead = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            history[i] = np.float32(mini_batch[i][0] / 255.)\n",
    "            next_history[i] = np.float32(mini_batch[i][3] / 255.)\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            dead.append(mini_batch[i][4])\n",
    "\n",
    "        value = self.model.predict(history)\n",
    "        target_value = self.target_model.predict(next_history)\n",
    "\n",
    "        # get max Q-value at s' from target model\n",
    "        for i in range(self.batch_size):\n",
    "            if dead[i]:\n",
    "                target[i] = reward[i]\n",
    "            else:\n",
    "                # key DDQN idea: select action from model, update value from target model\n",
    "                target[i] = reward[i] + self.discount_factor * target_value[i][np.argmax(value[i])]\n",
    "\n",
    "        loss = self.optimizer([history, action, target])\n",
    "        self.avg_loss += loss[0]\n",
    "\n",
    "    def setup_summary(self):\n",
    "        episode_total_reward = tf.Variable(0.)\n",
    "        episode_avg_max_q = tf.Variable(0.)\n",
    "        episode_duration = tf.Variable(0.)\n",
    "        episode_avg_loss = tf.Variable(0.)\n",
    "\n",
    "        tf.summary.scalar('Total Reward/Episode', episode_total_reward)\n",
    "        tf.summary.scalar('Average Max Q/Episode', episode_avg_max_q)\n",
    "        tf.summary.scalar('Duration/Episode', episode_duration)\n",
    "        tf.summary.scalar('Average Loss/Episode', episode_avg_loss)\n",
    "\n",
    "        summary_vars = [episode_total_reward, episode_avg_max_q,\n",
    "                        episode_duration, episode_avg_loss]\n",
    "        summary_placeholders = [tf.placeholder(tf.float32) for _ in\n",
    "                                range(len(summary_vars))]\n",
    "        update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in\n",
    "                      range(len(summary_vars))]\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        return summary_placeholders, update_ops, summary_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dtkachenko/.pyenv/versions/3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:84: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/Users/dtkachenko/.pyenv/versions/3.6.3/lib/python3.6/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 84, 84, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 20, 20, 32)   8224        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 9, 9, 64)     32832       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 7, 7, 64)     36928       conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 3136)         0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 512)          1606144     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          1606144     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            513         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            1539        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 3)            0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 3)            0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "merge_1 (Merge)                 (None, 3)            0           lambda_2[0][0]                   \n",
      "                                                                 lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,292,324\n",
      "Trainable params: 3,292,324\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 84, 84, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 20, 20, 32)   8224        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 9, 9, 64)     32832       conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 7, 7, 64)     36928       conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 3136)         0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 512)          1606144     flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 512)          1606144     flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            513         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 3)            1539        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 3)            0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 3)            0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "merge_2 (Merge)                 (None, 3)            0           lambda_4[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,292,324\n",
      "Trainable params: 3,292,324\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "INFO:tensorflow:Summary name Total Reward/Episode is illegal; using Total_Reward/Episode instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-02-11 01:58:11,678] Summary name Total Reward/Episode is illegal; using Total_Reward/Episode instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Average Max Q/Episode is illegal; using Average_Max_Q/Episode instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-02-11 01:58:11,681] Summary name Average Max Q/Episode is illegal; using Average_Max_Q/Episode instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Average Loss/Episode is illegal; using Average_Loss/Episode instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-02-11 01:58:11,684] Summary name Average Loss/Episode is illegal; using Average_Loss/Episode instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 1.0   memory length: 126   epsilon: 1.0   global_step: 126   average_q: -0.033501316542   average loss: 0.0\n",
      "episode: 1   score: 1.0   memory length: 264   epsilon: 1.0   global_step: 264   average_q: -0.0337682404434   average loss: 0.0\n",
      "episode: 2   score: 2.0   memory length: 464   epsilon: 1.0   global_step: 464   average_q: -0.0361336218007   average loss: 0.0\n",
      "episode: 3   score: 4.0   memory length: 764   epsilon: 1.0   global_step: 764   average_q: -0.0345563011989   average loss: 0.0\n",
      "episode: 4   score: 2.0   memory length: 960   epsilon: 1.0   global_step: 960   average_q: -0.0350767627967   average loss: 0.0\n",
      "episode: 5   score: 2.0   memory length: 1150   epsilon: 1.0   global_step: 1150   average_q: -0.0368856426917   average loss: 0.0\n",
      "episode: 6   score: 0.0   memory length: 1268   epsilon: 1.0   global_step: 1268   average_q: -0.0353898896631   average loss: 0.0\n",
      "episode: 7   score: 1.0   memory length: 1418   epsilon: 1.0   global_step: 1418   average_q: -0.0348487840593   average loss: 0.0\n",
      "episode: 8   score: 0.0   memory length: 1528   epsilon: 1.0   global_step: 1528   average_q: -0.0348871763796   average loss: 0.0\n",
      "episode: 9   score: 1.0   memory length: 1677   epsilon: 1.0   global_step: 1677   average_q: -0.0350994206215   average loss: 0.0\n",
      "episode: 10   score: 0.0   memory length: 1805   epsilon: 1.0   global_step: 1805   average_q: -0.035035471461   average loss: 0.0\n",
      "episode: 11   score: 0.0   memory length: 1908   epsilon: 1.0   global_step: 1908   average_q: -0.0352667588198   average loss: 0.0\n",
      "episode: 12   score: 1.0   memory length: 2064   epsilon: 1.0   global_step: 2064   average_q: -0.0359740517078   average loss: 0.0\n",
      "episode: 13   score: 1.0   memory length: 2188   epsilon: 1.0   global_step: 2188   average_q: -0.0333003297809   average loss: 0.0\n",
      "episode: 14   score: 0.0   memory length: 2288   epsilon: 1.0   global_step: 2288   average_q: -0.0353728515655   average loss: 0.0\n",
      "episode: 15   score: 0.0   memory length: 2411   epsilon: 1.0   global_step: 2411   average_q: -0.0349283220807   average loss: 0.0\n",
      "episode: 16   score: 2.0   memory length: 2615   epsilon: 1.0   global_step: 2615   average_q: -0.0330871743899   average loss: 0.0\n",
      "episode: 17   score: 2.0   memory length: 2788   epsilon: 1.0   global_step: 2788   average_q: -0.034870696464   average loss: 0.0\n",
      "episode: 18   score: 1.0   memory length: 2938   epsilon: 1.0   global_step: 2938   average_q: -0.0344642898937   average loss: 0.0\n",
      "episode: 19   score: 0.0   memory length: 3040   epsilon: 1.0   global_step: 3040   average_q: -0.0353459484656   average loss: 0.0\n",
      "episode: 20   score: 3.0   memory length: 3239   epsilon: 1.0   global_step: 3239   average_q: -0.0330386700006   average loss: 0.0\n",
      "episode: 21   score: 0.0   memory length: 3355   epsilon: 1.0   global_step: 3355   average_q: -0.0345592499422   average loss: 0.0\n",
      "episode: 22   score: 2.0   memory length: 3554   epsilon: 1.0   global_step: 3554   average_q: -0.0343922547547   average loss: 0.0\n",
      "episode: 23   score: 1.0   memory length: 3725   epsilon: 1.0   global_step: 3725   average_q: -0.034918591903   average loss: 0.0\n",
      "episode: 24   score: 2.0   memory length: 3921   epsilon: 1.0   global_step: 3921   average_q: -0.0347600728273   average loss: 0.0\n",
      "episode: 25   score: 0.0   memory length: 4030   epsilon: 1.0   global_step: 4030   average_q: -0.0352494133243   average loss: 0.0\n",
      "episode: 26   score: 1.0   memory length: 4180   epsilon: 1.0   global_step: 4180   average_q: -0.0350984790921   average loss: 0.0\n",
      "episode: 27   score: 1.0   memory length: 4351   epsilon: 1.0   global_step: 4351   average_q: -0.0354283212395   average loss: 0.0\n",
      "episode: 28   score: 2.0   memory length: 4552   epsilon: 1.0   global_step: 4552   average_q: -0.0369979340898   average loss: 0.0\n",
      "episode: 29   score: 0.0   memory length: 4674   epsilon: 1.0   global_step: 4674   average_q: -0.0352233407072   average loss: 0.0\n",
      "episode: 30   score: 1.0   memory length: 4814   epsilon: 1.0   global_step: 4814   average_q: -0.0338135077485   average loss: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d2a64d95a41d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         agent.avg_q_max += np.amax(\n\u001b[0;32m---> 40\u001b[0;31m             agent.model.predict(np.float32(history / 255.))[0])\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# check if the agent is 'dead' and update remaining lives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1781\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1782\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1783\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2352\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = DuelingDDQNAgent(action_size=3)\n",
    "\n",
    "scores, episodes, global_step = [], [], 0\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    dead = False\n",
    "    # 1 episode = 5 lives\n",
    "    step, score, start_life = 0, 0, 5\n",
    "    observed_state = env.reset()\n",
    "\n",
    "    # One of the ideas from DeepMind's papers: don't take any action for\n",
    "    # some random number of steps at the start of an episode\n",
    "    for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "        observed_state, _, _, _ = env.step(NO_ACTION)\n",
    "\n",
    "    state, history = get_initial_state_history(observed_state)\n",
    "\n",
    "    while not done:\n",
    "        if agent.render:\n",
    "            env.render()\n",
    "        global_step += 1\n",
    "        step += 1\n",
    "\n",
    "        # get action for the current history and go one step in environment\n",
    "        action = agent.get_action(history)\n",
    "\n",
    "        # model returns one of the three actions (from [0; 2]),\n",
    "        # but 'real' actions are 1 through 3, because 0 is no-op (no action, no ball)\n",
    "        real_action = action + 1\n",
    "\n",
    "        observed_state, reward, done, info = env.step(real_action)\n",
    "\n",
    "        # pre-process the observation --> history\n",
    "        next_state = preprocess(observed_state)\n",
    "        next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "        next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "\n",
    "        agent.avg_q_max += np.amax(\n",
    "            agent.model.predict(np.float32(history / 255.))[0])\n",
    "\n",
    "        # check if the agent is 'dead' and update remaining lives\n",
    "        if start_life > info['ale.lives']:\n",
    "            dead = True\n",
    "            start_life = info['ale.lives']\n",
    "\n",
    "        reward = np.clip(reward, -1., 1.)\n",
    "\n",
    "        # save the sample <s, a, r, s', d> to the replay memory and train model\n",
    "        agent.replay_memory(history, action, reward, next_history, dead)\n",
    "        agent.train_replay()\n",
    "\n",
    "        # every once in a while, update the target model\n",
    "        if global_step % agent.update_target_rate == 0:\n",
    "            agent.update_target_model()\n",
    "\n",
    "        score += reward\n",
    "\n",
    "        if dead:\n",
    "            dead = False\n",
    "\n",
    "            # reset history\n",
    "            _, history = get_initial_state_history(observed_state)\n",
    "        else:\n",
    "            history = next_history\n",
    "\n",
    "        # log progress to console and TensorBoard on episode end\n",
    "        if done:\n",
    "            if global_step > agent.train_start:\n",
    "                stats = [score, agent.avg_q_max / float(step), step, agent.avg_loss / float(step)]\n",
    "                for i in range(len(stats)):\n",
    "                    agent.sess.run(agent.update_ops[i], feed_dict={\n",
    "                        agent.summary_placeholders[i]: float(stats[i])\n",
    "                    })\n",
    "                summary_str = agent.sess.run(agent.summary_op)\n",
    "                agent.summary_writer.add_summary(summary_str, e + 1)\n",
    "\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                    len(agent.memory), \"  epsilon:\", agent.epsilon,\n",
    "                    \"  global_step:\", global_step, \"  average_q:\",\n",
    "                    agent.avg_q_max / float(step), \"  average loss:\",\n",
    "                    agent.avg_loss / float(step))\n",
    "\n",
    "            agent.avg_q_max, agent.avg_loss = 0, 0\n",
    "\n",
    "    if e % MODEL_CHECKPOINT_FREQUENCY == 0:\n",
    "        agent.model.save_weights(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
